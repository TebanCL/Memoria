\chapter{Introducción}
\label{cap:introduccion}

\section{Antecedentes y motivación}
\label{intro:motivacion}

\subsection{Motivación}
\label{intro:motivacion:motivacion}

Los desastres naturales en el país han sido frecuentes en los últimos años. Sólo por mencionar algunos de los más recientes y recordados: la erupción del volcán Chaitén (Mayo, 2008), terremoto en Tocopilla (Noviembre, 2007), terremoto Concepción (2010), incendio de las Torres del Paine (Diciembre, 2011), incendio en Valparaíso (Abril, 2014), erupción volcán Villarrica (Marzo, 2015), aluviones en el norte (Marzo, 2015) entre otros. Dependiendo de las características de la emergencia, surgen en la población diversos tipos de necesidades; alimentos, agua, luz eléctrica, refugio, rescate o comunicación. Muchas veces éstas pueden no ser detectadas por las autoridades, al menos, no de forma expedita, lo que no resulta beneficioso para las personas que intentan sobrellevar de la mejor manera posible la crisis y esto se complica aún más cuando la necesidad involucra una necesidad básica, como la falta de agua, donde la vida de los afectados puede correr riesgo. No es problema sólo para las autoridades, \cite{ChatoSurvey} señalan que el comportamiento humano ante crisis como éstas no es de quedarse esperando o huir en pánico, sino que intentan tomar decisiones rápidas en base a la información que conocen. Esto quiere decir que existe gente dispuesta a ayudar, aun siendo ellos los mismos afectados, pero no siempre disponen de la información necesaria para saber dónde apuntar sus esfuerzos. Sería útil, dado lo anterior, tener algún medio que concentre las necesidades que pueda tener una población dentro del país para acudir en su auxilio, posterior a la ocurrencia de una emergencia catastrófica como las mencionadas anteriormente.

\subsection{Estado del arte}
\label{intro:motivacion:arte}

Los tópicos que se tratan en esta sección son variados, se comenzará señalando desde donde inicia este trabajo, seguido de las impresiones de distintos autores respecto al trabajo en redes sociales (\textit{Twitter} específicamente), continuando con sistemas de procesamiento para flujos de información para concluir con la construcción de clasificadores para etiquetado de datos.\\

En el marco de las jornadas chilenas de la computación \cite{WladdimiroPMI} propusieron un modelo, desarrollado para el proyecto PMI USA1024, para detectar necesidades de la población ante escenarios de desastres naturales. En se propone un modelo basado en \textit{Yahoo! S4} donde haciendo uso del paradigma de procesamiento de \textit{streams} de datos se forma un grafo cuyos nodos (Elementos de procesamiento o PE, por sus siglas en inglés), dividen el procesamiento en pequeñas tareas fácilmente replicables para paralelizar el \textit{pepeline}. En esa ocación desarrollaron distintos tipos de operadores mencionados a continuación:

\begin{itemize}
\item Recolector: Haciedo uso de la API de \textit{Twitter} obtiene el \textit{stream} de datos del mismo. 
\item \textit{Scheduler}: discrimina cada \textit{tweet} según la categoría que pertenece (Información, agua, electricidad o alimento), mediante el uso de una bolsa de palabras y la distancia \textit{Hamming}.
\item Filtrado: Utiliza un clasificador \textit{Naïve Bayes} para identificar si un \textit{tweet} es subjetivo o no.
\item Relevancia: Identificar si una información es o no confiable haciendo uso de la cantidad de publicaciones del usuario, sus seguidores y a quienes sigue para estimar una reputación del autor.
\item Ranking: Hace uso de la información anterior, decidiendo a qué le entrega mayor importancia.
\end{itemize}

Los autores concluyeron basándose en la carga computacional la importancia de una replicación adecuada para distribuirla entre los PE, pero no fueron concluyentes en cuánto o qué nivel de replicación sería el adecuado o cuándo replicar.\\

Diversos autores, entre los que podemos mencionar a \cite{VanDeVoort}, \cite{EventDetectionInTwitter}, \cite{Maldonado}, han señalado las dificultades que se presentan al trabajar utilizando como entradas los estados públicos (\textit{tweet}) de los usuarios de \textit{Twitter}, dentro de las dificultades señaladas se encuentran, por ejemplo, el acceso a la información; si bien existen accesos públicos a la información éstos son restringidos tanto en cantidad como en tiempo: Este punto de acceso permite acceder a un 1\% de la información generada en un instante, es decir, por cada cien \textit{tweets} sólo podra accederse a uno de ellos. Sólo se permite realizar 180 consultas cada quince minutos (aproximadamente 12 consultas por minuto) y, en el caso de ser un usuario identificado, se aumenta a 450 consultas dentro del mismo intervalo de tiempo (aproximadamente 30 consultas por minuto). Por otro lado existe un punto de acceso pagado denominado \textit{FireHose} el cual entrega libre acceso a la información.\\

Por otro lado \cite{VanDeVoort} señalan que la dificultad radica en el hecho de que cualquier persona puede realizar publicaciones en esta red social, induciendo ruido en la información (considerando el ruido como toda información que aparece junto a la deseada, pero no aporta nueva), además de, al ser publicaciones de máximo 140 caracteres es complejo contextualizar el contenido.\\ 	
\textbf{Falta agregar parte de SPS}

Se consideraron tres \textit{frameworks} de computación distribuida: \textit{Apache Storm}, \textit{Apache Spark} y \textit{Apache S4}. El primero presenta una solución basado en el modelo \textit{MapReduce} que toma datos estructurados de la forma (clave, valor) para llevaros a una lista de valores y se usa típicamente para procesar grandes cantidades de datos en distintos nodos que pueden o no estar cercanos físicamente. Los otros dos presentan un modelo basado en el procesamiento de eventos en tiempo real. El problema particular que presenta \textit{S4} es la falta de avances en su desarrollo, el cual ha estado paralizado desde el año 2013.\\ 

\textit{Storm} es un \textit{framework} de computación distribuida para trabajar datos en tiempo real de múltiples fuentes de manera distribuida, tolerante a fallos y de alta disponibilidad. Su funcionamiento se divide en dos elementos: Por un lado existen los \textit{Spout}, encargados de recoger el flujo de entrada de datos, y en segundo, los denominados \textit{bolts}, encajados de procesamiento o transformación de los datos. Por recomendación un \textit{bolt} sólo ha de realizar una tarea. \textit{Storm} puede funcionar de dos formas: En modo \textit{cluster} o modo local; en este último se simula un \textit{thread} por nodo y es utilizado para realizar pruebas locales.\\

\textbf{Falta agregar parte de clasificacion}

\textbf{Mencionar desde donde se obtiene la taxonomia de necesidades (la de chato)}

	
\section{Descripción del problema}
\label{intro:problema}

Se requiere hacer uso de la información generada por la población en \textit{Twitter} para que, en caso de alguna emergencia de carácter nacional, prestar apoyo a las autoridades encargadas de la toma de decisiones, por ejemplo, darles a conocer en qué lugar en particular se requiere asistir a la población con un determinado tipo de ayuda según la necesidad que se presente. ¿Cómo puede usarse la información disponible en Twitter para que, en casos de emergencia, ésta sea útil para ir en directo beneficio de la población en la que se generó satisfaciendo la necesidad específica que presentan?

\section{Solución propuesta}
\label{intro:solucion}

Se propone una aplicación que estará recogiendo constantemente, en tiempo real, publicaciones desde \textit{Twitter} y analizando si corresponde o no a una necesidad existente en el conjunto de necesidades detectables y, en casos afirmativos, mostrarlas durante un intervalo de tiempo sobre un mapa geográfico del país haciendo uso de los metadatos asociados al tweet, en el caso en que se encuentren disponibles o hacer uso del contenido para inferir sus ubicaciones si es posible.\\

Al tratarse de una aplicación que recogerá grandes cantidades de información, el desempeño que ésta tendrá ha de ser considerado, por ello, se hará uso de un framework de computación distribuida para procesar las grandes cantidades de tweets de la manera más eficiente posible. Lo anterior quiere decir que la aplicación tendrá, internamente, forma de grafo dirigido; cada nodo de este grafo corresponderá a un operador por el que la información fluirá. Estos operadores serán aquellos que la literatura señale como los apropiados para el caso, por ejemplo: filtro de \textit{stopwords}, filtro de \textit{spam}, corrector ortográfico, detector de sentimientos, etcétera.\\

El grupo RESPOND de la Universidad de Santiago de Chile se ha adjudicado fondos para el desarrollo de un proyecto de dos años de duración el cual consiste en el desarrollo de una plataforma de streaming a escala nacional, enfocada en el procesamiento de datos en caso de crisis. Esta plataforma hará uso de la información generada por los usuarios en redes sociales como fuente de datos. Se espera que esta plataforma provea de herramientas para que cualquier persona pueda desarrollar nuevas aplicaciones para atender las diversas problemáticas que puedan existir cuando el país se enfrente a catástrofes.\\

Para ayudar a difundir la plataforma se requiere construir tres aplicaciones, una que apoye la coordinación de voluntarios, una segunda que difunda noticias y mensajes y, finalmente, una que permita detectar necesidades de la población, todas ellas al presentarse escenarios de catástrofes naturales.\\

En particular, para este trabajo, se espera atacar el problema de la detección de necesidades de la población y servir de apoyo para la construcción de la plataforma de streaming en relación a qué operadores se han de construir y cómo ha de estructurarse el sistema para operar sobre datos nacionales.


\section{Objetivos y alcance del proyecto}
\label{intro:objetivos}

\subsection{Objetivo general}
	Construir un sistema escalable para la detección de necesidades de la población en tiempo real para escenarios de desastre natural haciendo uso de \textit{Twitter}.

\subsection{Objetivos específicos}
\begin{enumerate}
\item	Implementar un método encargado de la recolección de tweets generados dentro del territorio nacional haciendo uso de la API pública de Twitter.
\item	Especificar la taxonomía de las necesidades que serán detectadas.
\item	Diseñar e implementar el clasificador de necesidades.
\item	Definir de los elementos de procesamiento para la construcción del sistema capaz de trabajar los datos obtenidos a gran escala.
\item	Implementar una arquitectura escalable que soporte la aplicación.
\item	Evaluar la aplicación bajo condiciones de alto tráfico como podría ser el caso de una emergencia nacional.
\end{enumerate}

\subsection{Alcances}
Se utilizarán las publicaciones de \textit{Twitter} para llevar a cabo el procesamiento de la información y no se considera, en el marco de este trabajo, el uso de una red social alternativa, no porque no sea posible, sino que con el motivo de acotar el problema.\\

Las necesidades que la aplicación detectará no serán todas del universo posible de necesidades existentes, sino de un subconjunto que se considere más importante tanto por el equipo que está trabajando en el proyecto FONDEF como por el profesor patrocinador de éste trabajo; agua, vivienda o luz eléctrica, por ejemplo. De esta forma se logra acotar el problema reduciendo la cantidad de categorías y permitir una mayor precisión en la clasificación (trabajos similares han bordeado una precisión entre el sesenta y ochenta por ciento, pero estos resultados van de la mano con la cantidad de datos utilizados para entrenar), entendiendo la precisión como la relación de elementos clasificados correcta o incorrectamente.\\

Se considera para la construcción del clasificador un subconjunto de un \textit{dataset} de cuatro millones setecientos mil \textit{tweets} recogidos desde \textit{Twitter} correspondientes al terremoto ocurrido el 2010 en Chile. Este conjunto de datos contiene mensajes en distintos idiomas y ha sido filtrada llegando a aproximadamente un millón y medio de tweets; de aquel conjunto se obtendrá un subconjunto para realizar el etiquetado y ser usado como datos de entrenamiento. Este trabajo no surge de la nada, busca mejorar un trabajo anterior hecho para un proyecto PMI. \\

La aplicación podrá ser probada en cuando a su \textit{performance} ante situaciones de gran carga como lo sería el caso de una emergencia, haciendo uso de \textit{JMeter}, herramienta escrita en Java diseñada para realizar tales labores que permitirá simular condiciones de alto tráfico dentro de la aplicación por medio del envío de peticiones a la aplicación.


\section{Metodología y herramientas utilizadas}
\label{intro:metodologia}

\subsection{Metodología}
Este trabajo considera dos partes, la primera es la generación del clasificador y la segunda la construcción de la aplicación, donde la segunda depende de haber completado la primera, por ello la principal prioridad será desarrollar este clasificador.\\

Para realizar el clasificador se tiene considerado el proceso de KDD \cite{KDDFayyad}, acrónimo de Knowledge Discovery in Databases o, simplemente, Descubrimiento (o extracción) de conocimiento en bases de datos. Se refiere al “proceso no-trivial de descubrir conocimiento, patrones e información potencialmente útiles dentro de los datos contenidos en algún repositorio” \cite{KDDDefinicion}. Este proceso iterativo diseñado para explorar grandes volúmenes de datos. Consta de cinco fases:\\

\begin{itemize}
\item	Selección de datos: Se determinan las fuentes de datos y el tipo de información a utilizar. Se extraen los datos útiles de las fuentes de datos.
\item	Pre-procesamiento: Los datos se preparan y limpian. Se utilizan estrategias para rellenar los datos en blanco o con información faltante. Finalmente en esta etapa se obtiene una estructura de datos adecuada para ser transformada, posteriormente.
\item	Transformación: Consiste en el tratamiento preliminar de datos, transformación y generación de nuevas variables a partir de las ya existentes. 
\item	Data Mining: Fase de modelamiento propiamente tal. Se utilizan métodos para obtener o detectar patrones que están “ocultos” en los datos.
\item	Interpretación y evaluación: Se identifican los patrones y se analizan por alguna métrica y se evalúan los resultados obtenidos.
\end{itemize}

En segundo lugar se tiene la aplicación propiamente tal que será dividida en dos, por un lado se tendrá la aplicación que llamaremos el núcleo que se encargará de recepcionar la información y el visualizador o interfáz que la mostrará por pantalla.\\

Para realizar lo anterior se hará el uso de \textit{Extreme Programming} (en adelante XP), presentando avances semanales y discutiendo cambios a realizar en la aplicación, tanto visuales como de funcionamiento interno.\\

Para manejar las tareas se hará uso de un tablero kanban de cuatro columnas: "Por hacer", "Haciendo", "Por Revisar" y "Completo" donde una tarea sólo podrá considerarse completa habiendo pasado por la revisión y haber sido aceptada.

\subsection{Herramientas de desarrollo}
\label{subsec:HerrDesarrollo}

\subsubsection{Herramientas de Software}
\label{subsubsec:HerrSoftware}

Se han se utilizar las siguientes herramientas de software para la construcción de la aplicación:\\

\begin{itemize}
\item Java como lenguaje de programación.
\item Apache Storm (1.0.1), como \textit{framework} de computación distribuida.
\item Apache Zookeeper (3.4.8), como herramienta para mantener la configuración
\item Mallet (2.0.7), como herramienta de \textit{Data Mining} para la construcción del clasificador.
\item MongoDB (3.2.6), para la persistencia de datos.
\item Play Framework (2.5.3), como \textit{framework} para el desarrollo de aplicaciones Java. En particular, la construcción de la aplicación que permitirá visualizar los datos. 
\item Sublime Text 3 (Build 3103), como editor de textos.
\item MiKTex (XeLaTeX), para la escritura de la memoria.
\item PowerDesigner 16, para la elaboración de diagramas.
\item Bitbucket (Git), como repositorio de todo lo referente al proyecto (Núcleo, Visualizador y Memoria).
\item Windows 10 Home Edition (x64).
\item Linux Mint 17.3 (x86).
\item Oracle VirtualBox (5.0.14).
\end{itemize}

\subsubsection{Herramientas de hardware}
\label{subsubsec:HerrHardw}

Se utilizará el equipo del autor cuyas características son las siguientes:\\
\begin{itemize}
\item Procesador Intel Core i5 2.2 Ghz.
\item 8 GB de memoria RAM.
\item 1 TB de disco duro.
\end{itemize}

\section{Organización del documento}
\label{intro:organizacion}

A continuación se presentan a grueso modo los capítulos que componen el presente documento:\\

El capítulo de ~\nameref{cap:MarcTeorico} presenta una serie de definiciones detalladamente para ayudar a comprender de mejor manera el problema y los elementos utilizados para su resolución.\\

El capítulo de ~\nameref{cap:Construccion} detallan tanto los aspectos de toma de requerimientos y diseño de la aplicación detallando las desiciones que se tomaron para solucionar los problemas encontrados en el desarrollo del la aplicación como el proceso de implementación.\\

El capítulo de ~\nameref{cap:Evaluacion} detalla cómo se evaluó la solución y el por qué se decidió una topología en particular y su nivel de replicación de operadores.\\

Finalmente en el capítulo de ~\nameref{cap:Conclusiones} presenta las conclusiones del trabajo realizado, el cumplimiento de objetivos y trabajo futuro.\\