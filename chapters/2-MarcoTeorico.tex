\chapter{Marco Teórico y estado del arte}
\label{cap:MarcTeorico}

Este capítulo busca realizar una contextualización de los conceptos necesarios para entender el problema que se está tratando por medio de una breve reseña de cada uno, además de a conocer al lector lo último en las áreas que el presente trabajo está inmerso. 

\section{Minería de datos}
\label{sec:dateMine}

A veces llamada como "descubrimiento de información o conocimiento", es el proceso de analizar información de diferentes perspectivas y transformarlo en información de utilidad. Puede ser aplicado a distintas fuentes de datos como: bases de datos, imágenes, internet, etc. Es un campo multidiciplinal que involucra el aprendizaje de máquina, la estadística, bases de datos, la inteligencia artificial y la recuperación de información.

Siendo distintos los usos que pueden dársele se pueden generalizar cuatro etapas:

\begin{itemize}
\item \textbf{Determinación de objetivos}: Delimitar los objetivos que se esperan alcanzar con el proceso de minado.
\item \textbf{Preprocesamiento de datos}: Se refiere a la limpieza, reducción y transformación de las bases de datos. Es, generalmente, el subproceso que utiliza la mayor cantidad de tiempo.
\item \textbf{Determinación del modelo}: Aplicación de algoritmos para generar un modelo que cumpla los objetivos planteados. Se genera nuevo conocimiento o se descubre un patrón.
\item \textbf{Análisis de los resultados}: Se verifica si el conocimiento es útil.
\end{itemize}

Dentro de las tareas que pueden realizarse utilizando \textit{data mining} pueden encontrarse tales como: Aprendizaje supervisado o clasificación, no supervisado o clustering y reglas de asociación.

	\subsection{Minería de la Web}
	\label{subsec:webMine}
	La aplicación de la minería de datos al contenido que se encuentra en línea es conocida como Minería de la \textit{web} o \textit{web mining}. Se diferencia de la minería de datos tradicional en que ésta última utiliza repositorios de datos; en cambio, la minería \textit{web}, hace uso de información extraída directamente desde la \textit{web}.\\
	Sus métodos son similares en cuanto a sus etapas:
	\begin{itemize}
	\item Selección de las fuentes: referencia al proceso de recuperación de los datos.
	\item Selección y pre-procesamiento: Incluye cualquier transformación o pre-procesamiento que puedan realizárseles a los datos, por ejemplo, eliminar elementos, como palabras, aplicación de correctores de datos, etc.
	\item Generalización: Etapa donde se realiza el proceso de minería en sí. 
	\item Análisis: Desarrolla técnicas para utilizar o visualizar el conocimiento adquirido.
	\end{itemize} 

	La información obtenida puede ser utilizara para analizar tanto el contenido de la web (\textit{Web content mining}) como sus enlaces (o relaciones) (\textit{Web  structure mining}) y/o el registro de navegación de los usuarios (\textit{Web usage mining}).

	La primera se refiere a búsqueda entre documentos \textit{web} (texto o imágenes), es decir, analiza los documentos y no la relación entre ellos.

	La segunda se dedica a analizar la topología de los vínculos existentes y/o analizar la estructura interna de la página \textit{web} y descrbir el \textit{HTML} o el \textit{XML} de la misma.

	En particular dentro de la minería de contenido encontramos la minería de texto o \textit{text mining}. Ésta tiene como objetivo el descubrir nueva información a partir colecciones de documentos de texto no estructurado, es decir, texto libre (lenguaje natural, generalmente), aunque también es aceptable otro tipo de información textual como un código fuente. Lo más habitual es trabajar el texto para categorizarlo (Asignar una o más categorías a un documento), clasificarlo (Asignar sólo una clase a un documento) y/o agruparlo (organizar en torno a una jerarquía basado en alguna similitud).

	El primer paso para comenzar a trabajar haciendo uso de minería de texto es representar los datos de alguna manera para luego dárselo a los algoritmos adecuados. Algunas de estas representaciones pueden ser las siguientes:

	\begin{itemize}
	\item Bolsas de palabras (\textit{Bag of words}): Representar el texto como un vector de largo n, donde n corresponde al número de palabras, así cada palabra corresponde a un elemento del vector.
	\item Frases: Considera el texto, simplemente, como una frase sintáctica. Así se permite conservar el contexto.
	\item N-gramas: Consideran la información de la posición de la palabra en el texto mediante secuencias de longitud n (n-gramas). 
	\end{itemize}

	Habiendo realizado la representación, el paso siguiente es reducir el conjunto de características. La literatura indica que los métodos más frecuentados son la eliminación de palabras que no aportan información, llevar las palabras a una palabra raíz (\textit{stemming}), entre otros. \cite{DMPreprocessing}.

\subsection{Aprendizaje supervisado}
\label{subsec:aprendSuperv}

Se caracteriza por ser un proceso de aprendizaje en el que éste se realiza mediante un entrenamiento controlado por un agente externo, el que determina qué respuesta debería generarse a partir de una entrada determinada. \cite{AprendizajeSupervisado}.

Se asocia al concepto de \textit{machine learning} con la minería de datos; la primera busca patrones conocidos y predecir en base a ellos mientras que la segunda busca patrones con anterioridad desconocidos, es decir, la primera tiene una función focalizada en la predicción mientras que la segunda realiza una función exploratoria.

Los datos son denominados instancias, ejemplares, casos o vectores, donde una instancia corresponde a cada uno de los datos disponibles para el análisis.

Los datos poseen atributos son los elementos dentro de las instancias. Una instancia puede tener asociado un elemento de otro conjunto de atributos llamado "Clase", correspondiente a etiquetas de identificación.

Teniendo en cuenta los elementos vistos con anterioridad, se define el objetivo del proceso de aprendizaje como construir una función que relacione las instancias con las clases llamada modelo o, en este caso, clasificador.

Se le llama conjunto de entrenamiento al conjunto de datos utilizado para el aprendizaje. Este conjunto es entregado como entrada al algoritmo de aprendizaje y construcción del modelo. Para realizar la evaluación de la calidad del modelo se utiliza un segundo conjunto de instancias llamado datos de validación. Se espera que estos datos no hayan sido vistos con anterioridad por máquina y así obtener la confianza, es decir, la probabilidad de acierto que calcula el sistema para cada predicción.

Lo ventajoso de este método es que se podrá clasificar una instancia sin haberla visto nunca, pero la desventaja principal es la que han de utilizarse una gran cantidad de instancias para el proceso de entrenamiento. El proceso de entrenamiento y evaluación se ilustra en la Figura ~\ref{fig:entrenamientoEvaluacion}.

\begin{figure}[H]
	\centering
	\captionsetup{justification=centering}
	\includegraphics[scale=0.6]{images/trainTest.png}
	\caption[Proceso de entrenamiento y prueba del modelo.]{Proceso de entrenamiento y prueba del modelo.}
	\label{fig:entrenamientoEvaluacion}
\end{figure}

Dentro de los algoritmos utilizados para la construcción de clasificadores se encuentra \textit{Naïve Bayes} \cite{NaiveBayes2} a continuación se realiza una descripción de este. 

	\subsubsection{Naïve Bayes}
	\label{subsubsec:naiveBayes}
	
	La clasificación puede verse como una función \begin{math}\gamma\end{math} que asigna etiquetas a observaciones \cite{NaiveBayes1}, es decir:

	\[\gamma : (x_{1}, .... , x_{n}) \rightarrow \{1, 2, .... r_{0}\} \]

	Existe una matriz de costo \begin{math} cos(r, s)\end{math} con \begin{math} r, s = 1, ...., r_{0}\end{math} en el cual se refleja el costo asociado a las clasificaciones incorrectas. En concreto \begin{math} cos(r, s) \end{math} indica el costo de clasificar un ejemplo de la clase r como de la clase s. En el caso especial de la función de pérdida 0/1, se tiene:

	\[ cos(r, s) = \left \{ \begin{matrix} 0 & \mbox{si }r \ne s
\\ 1 & \mbox{si }r = s \end{matrix}\right. \]

	Subyacente a las observaciones suponemos la existencia de una distribución de probabilidad conjunta:

	\[
		p(x_{1}, ..., x_{n}, c) = p(c | x_{1}, ..., x_{n})p(x_{1}, ..., x_{n}) = p(x_{1}, ..., x_{n}|c)p(c)
	\]

	La cual es desconocida. El objetivo es construir un clasificador que miniza el coste total de los errores cometidos, y esto se consigue, \cite{NaiveBayes3} por medio del clasificador de Bayes:

	\[
		\gamma(x) = arg min_{\substack{k}} \sum^{r_{0}}_{\substack{c = 1}} cos(k,c)p(c | x_{1}, ..., x_{n})
	\]

	En el caso que la función de pérdida sea 0/1, el clasificador de Bayes se convierte en asignar al ejemplo \begin{math} x = (x_{1}, ..., x_{n})\end{math} la clase con mayor probabilidad a posteriori. Es decir:

	\[
		\gamma(x) = arg max_{\substack{c}} p(c | x_{1}, ..., x_{n})
	\]

	En la práctica la función de distribución conjunta \begin{math} p(x_{1}, ..., x_{n}, c) \end{math} es desconocida, y puede ser estimada a partir de una muestra aleatorea simple \begin{math} \{ (x^{(1)}, c^{(1)} ), ..., (x^{(N)}, C^{(N)})\}\end{math} extraida de dicha función de distribución conjunta.

	El paradigma clasificatorio en el que se utiliza el teorema de Bayes en conjunción con la hipótesis de independencia condicional de las variables predictorias dada la clase se conoce como Naïve Bayes, \cite{NaiveBayes3}.

	Finalmente el Teorema de Bayes está representado por la expresión \cite{NaiveBayes4}:

	\[
		P(c_{i}|d_{j}) = \frac{P(d_{j}|c_{i}) · P(c_{i})}{P(d_{j})}
	\]

	Donde \begin{math} c_{i} \end{math} corresponde al atributo Clase y \begin{math} d_{j} \end{math} al conjunto de documentos. El término \begin{math} P(d_{j}) \end{math} suele omitirse, pues no aporta mucha información para la clasificación. Habiendo realizado lo anterior y tomando en cuenta la hipótesis de independencia se obtiene:

	\[
		P(d_{j}|c_{i}) = P(c_{i})\prod_{j=1}^n P(a_{j}|c_{i})
	\]

	Pero siempre se considera la mayor probabilidad de \begin{math} c_{i} \end{math}, por ello podemos añadir un nuevo elemento llegando a la fórmula de Naïve Bayes:

	\[
		P(d_{j}|c_{i}) = ArgMax{\substack{k}^n P(c_{i})}\prod_{j=1}^n P(a_{j}|c_{i})
	\]

	En términos simples el crear un modelo de clasificación Naïve Bayes se puede resumir en el Algoritmo \ref{alg:NaiveB}.

	\begin{algorithm}[H]\setstretch{1.5}
	\begin{algorithmic}[numeracion_lineas]
		\REQUIRE Clases $C=\{c_{0}, \dots, c_{n} \}$.
		\REQUIRE Datos de entrenamiento $D= \{d_{0}, \dots, d_{n} \}$
		\ENSURE Modelo de clasificación $M$.

		\FOR{Clase $c_{i}$ perteneciente a $C$}
			\STATE Calcular las probabilidades a priori para cada clase (Probabilidad de que un dato cualquiera pertenezca a $c_{i}$.
		\ENDFOR

		\FOR{Clase $c_{i}$ perteneciente a $C$}
			\STATE Realizar un recuento de los valores $v_{i}$ de atributos que toma cada ejemplo $d_{i}$, guardarlos en $V$.
		\ENDFOR

		\FOR{Valores $v_{i}$ perteneciente a $V$}
			\STATE Aplicar corrección de Laplace a $v_{i}$.
		\ENDFOR

		\FOR{Valores $v_{i}$ perteneciente a $V$}
			\STATE Normalizar para obtener un rango de valores [0,1].
		\ENDFOR

	\end{algorithmic}
	\caption{Algoritmo Naïve Bayes para la construcción del clasificador.}
	\label{alg:NaiveB}
	\end{algorithm}\vphantom\\

	Para utilizar el modelo generado por el Algoritmo \ref{alg:NaiveB}, se hace uso del Algoritmo \ref{alg:NaiveBAplic} sobre un dato de entrada $d_{j}$.

	\begin{algorithm}[H]\setstretch{1.5}
	\begin{algorithmic}[numeracion_lineas]
		\REQUIRE Clases $C=\{c_{0}, \dots, c_{n} \}$.
		\REQUIRE Dato $d_{j}$.
		\ENSURE Etiqueta del Dato $d_{i}$.

		\FOR{Clase $c_{i}$ perteneciente a $C$}
			\STATE Calcular las probabilidades a priori para cada clase (Probabilidad de que un dato cualquiera pertenezca a $c_{i}$.
		\ENDFOR

		\STATE retornar $ArgMax{\substack{k}^n P(c_{i})}\prod_{j=1}^n P(d_{j}|c_{i})$.

	\end{algorithmic}
	\caption{Algoritmos Naïve Bayes para la aplicación de un modelo clasificador.}
	\label{alg:NaiveBAplic}
	\end{algorithm}\vphantom\\

\section{Estado del arte}
\label{intro:motivacion:arte}

Los tópicos que se tratan en esta sección son variados, se comienza señalando desde donde inicia este trabajo, seguido de las impresiones de distintos autores respecto al trabajo en redes sociales (\textit{Twitter} específicamente), continuando con sistemas de procesamiento para flujos de información para concluir con la construcción de clasificadores para etiquetado de datos.

En el marco de las jornadas chilenas de la computación \cite{WladdimiroPMI} propusieron un modelo, desarrollado para el proyecto PMI USA1024, para detectar necesidades de la población ante escenarios de desastres naturales. En se propone un modelo basado en \textit{Yahoo! S4} donde haciendo uso del paradigma de procesamiento de \textit{streams} de datos se forma un grafo cuyos nodos (Elementos de procesamiento o PE, por sus siglas en inglés), dividen el procesamiento en pequeñas tareas fácilmente replicables para paralelizar el \textit{pepeline}. En esa ocación desarrollaron distintos tipos de operadores mencionados a continuación:

\begin{itemize}
\item Recolector: Haciedo uso de la API de \textit{Twitter} obtiene el \textit{stream} de datos del mismo. 
\item \textit{Scheduler}: discrimina cada \textit{tweet} según la categoría que pertenece (Información, agua, electricidad o alimento), mediante el uso de una bolsa de palabras y la distancia \textit{Hamming}.
\item Filtrado: Utiliza un clasificador \textit{Naïve Bayes} para identificar si un \textit{tweet} es subjetivo o no.
\item Relevancia: Identificar si una información es o no confiable haciendo uso de la cantidad de publicaciones del usuario, sus seguidores y a quienes sigue para estimar una reputación del autor.
\item Ranking: Hace uso de la información anterior, decidiendo a qué le entrega mayor importancia.
\end{itemize}

Los autores concluyeron basándose en la carga computacional la importancia de una replicación adecuada para distribuirla entre los PE, pero no fueron concluyentes en cuánto o qué nivel de replicación es el adecuado o cuándo replicar.

\subsection{La problemática de \textit{Twitter}}
\label{intro:ea:probTwitter}

Diversos autores, entre los que podemos mencionar a \cite{VanDeVoort}, \cite{EventDetectionInTwitter}, \cite{Maldonado}, han señalado las dificultades que se presentan al trabajar utilizando como entradas los estados públicos (\textit{tweet}) de los usuarios de \textit{Twitter}, dentro de las dificultades señaladas se encuentran, por ejemplo, el acceso a la información; si bien existen accesos públicos a la información éstos son restringidos tanto en cantidad como en tiempo: Este punto de acceso permite acceder a un 1\% de la información generada en un instante, es decir, por cada cien \textit{tweets} sólo podra accederse a uno de ellos. Sólo se permite realizar 180 consultas cada quince minutos (aproximadamente 12 consultas por minuto) y, en el caso de ser un usuario identificado, se aumenta a 450 consultas dentro del mismo intervalo de tiempo (aproximadamente 30 consultas por minuto). Por otro lado existe un punto de acceso pagado denominado \textit{FireHose} el cual entrega libre acceso a la información.

Por otro lado \cite{VanDeVoort} señalan que la dificultad radica en el hecho de que cualquier persona puede realizar publicaciones en esta red social, induciendo ruido en la información (considerando el ruido como toda información que aparece junto a la deseada, pero no aporta nueva), además de, al ser publicaciones de máximo 140 caracteres es complejo contextualizar el contenido.

\subsection{Procesamiento de la información}
\label{intro:ea:procesamiento}

Para procesar datos, como los del \textit{stream} de \textit{Twitter}, donde los datos llegan en ráfagas, pueden ser de gran o poco volúmen y requieren de una respuesta inmediata, \cite{HarwoodPeter},hace falta un cambio de paradigma respecto al procesamiento \textit{batch} o procesamiento por lotes, donde un programa ejecuta procesos sin la intervención de terceros desde una base de datos o fichero, por uno donde los datos sean continuos y sin final conocido.
 
Se consideraron tres \textit{frameworks} de computación distribuida: \textit{Apache Storm}, \textit{Apache Spark} y \textit{Apache S4}. El primero presenta una solución basado en el modelo \textit{MapReduce} que toma datos estructurados de la forma (clave, valor) para llevaros a una lista de valores y se usa típicamente para procesar grandes cantidades de datos en distintos nodos que pueden o no estar cercanos físicamente. Los otros dos presentan un modelo basado en el procesamiento de eventos en tiempo real. El problema particular que presenta \textit{S4} es la falta de avances en su desarrollo, el cual ha estado paralizado desde el año 2013. 

\textit{Storm} es un \textit{framework} de computación distribuida para trabajar datos en tiempo real de múltiples fuentes de manera distribuida, tolerante a fallos y de alta disponibilidad. Su funcionamiento se divide en dos elementos: Por un lado existen los \textit{Spout}, encargados de recoger el flujo de entrada de datos, y en segundo, los denominados \textit{bolts}, encajados de procesamiento o transformación de los datos. Por recomendación un \textit{bolt} sólo ha de realizar una tarea. \textit{Storm} puede funcionar de dos formas: En modo \textit{cluster} o modo local; en este último se simula un \textit{thread} por nodo y es utilizado para realizar pruebas locales.

\subsection{Clasificación de textos}
\label{intro:ea:clasificacion}

Por otra parte, la clasificación de texto en servicios de \textit{microblogging}, como \textit{Twitter} es un problema cuya solución tiene diferentes puntos de vista, los métodos tradicionales incluyen hacer uso de una bolsa de palabras para clasificar según el contenido del texto, construcción de n-gramas para clasificar según términos co-ocurrentes o ubicar el texto en una categoría haciendo uso de técnicas de aprendizaje de máquina o \textit{Machine Learning}, \cite{EventDetection}. Éste último método ya ha sido comprobado por diversos autores, entre ellos \cite{Maldonado}, quien utilizó este método para realizar su memoria donde clasificaba \textit{tweets} según sentimientos positivos, negativos o neutros. De igual forma \cite{WladdimiroPMI} utilizaron en su trabajo un clasificador basado en \textit{machine learning} para verificar la subjetividad de un \textit{tweet}, por lo que ya esta demostrado que esta herramienta es capaz de categorizar texto, por lo que puede ser aplicada para las entradas de \textit{Twitter}.
	

	