\chapter{Conclusiones}
\label{cap:conclusiones}

\textit{Twitter}, al ofrecer a los desarrolladores un punto de acceso a la información que está fluyendo por la aplicación en tiempo real, entrega una potente herramienta para la búsqueda de información, pues sus datos pueden ser objeto de análisis en diferentes áreas como por ejemplo: búsqueda de tendencias, política, eventos, entre otros. Lo que hace a \textit{Twitter} tan especial es que cualquier persona puede acceder a él, no en vano cuenta con millones de usuarios en todo el mundo y mediante su API posibilita obtener información generada por sus usuarios. 

En emergencias es importante una rápida toma de decisiones para así reducir el número de víctimas y distribuir de mejor manera los recursos disponibles; la información de redes sociales como \textit{Twitter} puede ser de mucha ayuda para ello. Teniendo esto en consideración, se propuso una herramienta de apoyo a la gestión de desastres capaz de identificar y posicionar geoespacialmente la información expresada por los usuarios. Esta herramienta identifica y clasifica necesidades expresadas en texto de manera de poder priorizar la ayuda en aquellas zonas que presenten necesidades de mayor relevancia. Para ello se utilizaron técnicas de aprendizaje de máquina, en este caso, minería de textos. Ésta se diferencia de la minería tradicional dado que su fuente de información está en documentos y no en bases de datos. 

En el caso de \textit{Twitter} se presenta un desafío, pues su información (el texto del \textit{tweet}) no está estructurado, es decir, puede tener cualquier formato dentro de sus 140 caracteres. Para solucionar el inconveniente antes planteado se utilizó la metodología KDD que, mediante sus cinco etapas permitió la construcción de un clasificador de texto utilizando el algoritmo de \textit{Naïve Bayes} con el cual es posible etiquetar nuevos datos para la detección de necesidades. Si bien, como se mencionó en la sección \ref{sec:EvalClassificador}, el clasificador puede no ser el óptimo, pues aunque presenta gran precisión, no es exhaustivo al momento de clasificar (bajo \textit{recall}). La necesidad de tener un cuerpo de entrenamiento con gran cantidad de instancias fue un problema, debido a que el tiempo utilizado para etiquetar datos fue mayor al esperado, para no retrasar el desarrollo se optó por no etiquetar nuevos datos para incrementar el tamaño del conjunto de entrenamiento y en su lugar, buscando solventar esta debilidad del sistema, se construyó un módulo que permitiese la actualización del modelo de clasificación.

El sistema de detección de necesidades consistió en dos aplicaciones comunicadas por medio de la base de datos, posibilitando el manejo de grandes cantidades de datos sin que colapse la memoria del equipo.

Sobre los objetivos específicos es posible señalar lo siguiente con respecto a su completitud:

\begin{itemize}
\item Se implementó un \textit{spout} capaz de obtener el \textit{stream} de \textit{Twitter} para seleccionar aquellas que hicieran referencia al territorio nacional se utilizó un segundo operador, el detector de ubicación.
\item Se inició con el trabajo realizado por \citep{TaxonomiaChato}, pero finalmente y avalado por el equipo de trabajo del proyecto FONDEF IDeA, se decidió utilizar una modificación de la categorización realizada por \citep{PMIProfes}.
\item Se construyó un cuerpo de entrenamiento, con el cual se diseñó un clasificador bayesiano haciendo uso de la herramienta Mallet.
\item Mediante \textit{Apache Storm} se definieron 8 elementos, además del \textit{spout} para obtener los datos, con los cuales se dio soporte al sistema de detección de necesidades con capacidad de procesar grandes cantidades de datos.
\item La escalabilidad está dada por el uso de \textit{Apache Storm}, pues la aplicación de detección es aquella que realiza todo el procesamiento del sistema; en tanto la aplicación visualizadora se rige de los tiempos de respuesta de la base de datos. La rapidez en las respuestas a las consultas realizadas a la base de datos de MongoDB permiten que no se produzcan respuestas lentas al manejar un gran volumen de datos.
\item Se simuló una condición en tiempo real de la aplicación haciendo uso de los datos recopilados en el terremoto de Concepción del año 2010, presentando en la sección \ref{sec:AltoTrafico} los resultado de esta simulación. En pruebas realizadas no se vio influencia negativa dada la cuota máxima de accesos impuesta por \textit{Twitter}, pero es un punto que se ha de tener en cuenta al desplegar la aplicación para funcionar en modo producción, pues las esperas pueden significar datos perdidos.
\end{itemize}

Finalmente, señalar que por medio de la concreción de los objetivos específicos señalados, el sistema de detección de necesidades está completo y el objetivo general: ``Construir un sistema escalable para la detección de necesidades de la población en tiempo real para escenarios de desastre natural haciendo uso de Twitter." se completó, presentando el sistema listo recolectar los datos desde \textit{Twitter} y detectar los eventos categorizados como necesidades que allí aparezcan.

Habiendo concluido el desarrollo de la aplicación, queda como trabajo futuro los siguientes puntos: Por un lado, la elaboración de un conjunto de datos de entrenamiento más grande y cuyos elementos por clases estén balanceados como señala la teoría, para permitir generar un mejor modelo de clasificación. Por otro lado, la implementación de una API que dinámicamente encuentre los sinónimos de los términos de búsqueda para expandir de mejor manera las consultas, en lugar de hacer uso de un diccionario de sinónimos estáticos con sólo ciertos términos.

Es importante señalar que aunque se utilizó para el desarrollo de este trabajo un conjunto de datos correspondientes al terremoto de febrero del año 2010, el sistema no sólo sería capaz de detectar necesidades en sólo en eventos sismológicos, sino que con el modelo adecuado debiese ser capaz de detectar necesidades en cualquier catástrofe natural.

	





